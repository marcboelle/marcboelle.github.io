<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> fairness with Wasserstein barycenters | Marc Boëlle </title> <meta name="author" content="Marc Boëlle"> <meta name="description" content="&lt;a href='https://arxiv.org/abs/2006.07286'&gt;Fair Regression with Wasserstein Barycenters&lt;/a&gt; by Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, and Massimiliano Pontil"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/github-bleu.ico?373e275fcf98f791e007f24c2dbe501e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://marcboelle.github.io/blog/2023/fairness/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.post .distill d-contents nav{position:sticky;top:20px;max-height:calc(100vh - 40px);overflow-y:auto}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "fairness with Wasserstein barycenters",
            "description": "<a href='https://arxiv.org/abs/2006.07286'>Fair Regression with Wasserstein Barycenters</a> by Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, and Massimiliano Pontil",
            "published": "December 05, 2023",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Marc</span> Boëlle </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>fairness with Wasserstein barycenters</h1> <p><a href="https://arxiv.org/abs/2006.07286" rel="external nofollow noopener" target="_blank">Fair Regression with Wasserstein Barycenters</a> by Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, and Massimiliano Pontil</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#results-of-the-paper">Results of the paper</a> </div> <ul> <li> <a href="#problem-setting">Problem setting</a> </li> <li> <a href="#demographic-parity-dp">Demographic Parity (DP)</a> </li> <li> <a href="#strategy-of-the-authors">Strategy of the authors</a> </li> </ul> <div> <a href="#insurance-dataset">Insurance dataset</a> </div> <ul> <li> <a href="#data-visualization">Data visualization</a> </li> <li> <a href="#training-a-first-unfair-predictor">Training a first unfair predictor</a> </li> </ul> <div> <a href="#income-dataset">Income dataset</a> </div> </nav> </d-contents> <p>[work in progress…]</p> <p>When training a Machine Learning model, we make our predictor learn from the data which can be biased. For instance, if we want to decide how much an applicant would be paid, there will be a difference (in the reference data) between the wages of men and comen. But ideally, So the question is : how can we build a fair predictor when the training data is biased?</p> <h2 id="results-of-the-paper">Results of the paper</h2> <h3 id="problem-setting">Problem setting</h3> <p>The authors focus on regression problems \(Y=f^*(X,S)+\xi\), where \(f^*\) is the regression function minimizing the squared risk, \(S\) a sensitive attribute and \(\xi\) a (centered) noise.</p> <h3 id="demographic-parity-dp">Demographic Parity (DP)</h3> <p>To evaluate the fairness of some predictors, there are different metrics available. Demographic Parity is one of them and extensively used in the literature, in particular in this paper. Intuitively, it is satisfied if the distribution of the prediction is independent of the sensitive attribute.</p> <p>Formally, with \(S\) the attribute representing the sensitive groups in the population, a predictor \(g\) is fair (according to the DP criterion) if</p> \[\forall (s,s')\in S^2,\:\:\sup_{t\in\mathbb{R}}\left|\mathbb{P}(g(X,S)\leq t|S=s)-\mathbb{P}(g(X,S)\leq t|S=s')\right|=0\] <p>In other words, the Kolmogorov-Smirnov distance between the distributions on all sensitive groups must be \(0\).</p> <h3 id="strategy-of-the-authors">Strategy of the authors</h3> <p>From the optimal predictor \(f^*\), the authors suggests that we build a fair predictor \(g^*\) (w.r.t. DP) which remains “close” to \(f^*\) - to keep the accuracy as high as possible. We end up with this new problem :</p> \[\min_{g\text{ is fair}}\mathbb{E}\left(f^*(X,S)-g(X,S)\right)^2\] <p>Using Optimal Transport theory, the authors provide a closed form expression of \(g^*\) using Wasserstein barycenters :</p> <p>Theorem:</p> \[\min_{g \text{ is fair}} \mathbb{E}(f^*(X,S) - g(X,S))^2 = \min_{\nu} \sum_{s \in S} p_s \mathcal{W}_2^2(\nu_{f^*|s}, \nu)\] \[g^*(x,s) = \left( \sum_{s' \in S} p_{s'} Q_{f^*|s'} \right) \circ F_{f^*|s}(f^*(x,s))\] <p>with</p> \[\mathcal{W}_2^2(\mu, \nu) = \inf_{\gamma \in \Gamma_{\mu, \nu}} \int_{\gamma \in \Gamma_{\mu, \nu}} |x - y|^2 d\gamma(x,y)\] <table> <tbody> <tr> <td>with $$F_{f</td> <td>s}\(the CDF of\)f\(,\)Q_{f</td> <td>s}\(its quantile function and\)p_s = \mathbb{P}(S=s)$$.</td> </tr> </tbody> </table> <h2 id="insurance-dataset">Insurance dataset</h2> <h3 id="data-visualization">Data visualization</h3> <p>The first dataset we worked on is the insurance dataset. Each row corresponds to an individual, and our goal is to predict the amount of charges people have to pay for medical insurance. The sensitive groups we chose to consider for fairness evaluation are smokers and non-smokers. You may guess that smokers will have to pay higher charges for medical insurance than non-smokers. And indeed they do:</p> <div class="row justify-content-center"> <div class="col-sm-8"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/insurance-dataset-480.webp 480w,/assets/img/fairness/insurance-dataset-800.webp 800w,/assets/img/fairness/insurance-dataset-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/fairness/insurance-dataset.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Of all the plots we made to visualize the data, we liked the following one :</p> <div class="row justify-content-center"> <div class="col-sm-8"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/insurance-viz-480.webp 480w,/assets/img/fairness/insurance-viz-800.webp 800w,/assets/img/fairness/insurance-viz-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/fairness/insurance-viz.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>It’s a scatter plot for charges versus BMI (Body Mass Index), and we’ve colored the points according to the sensitive attributes – smokers are in red, and non-smokers are in blue. You can clearly see the bias in the data, the red points being mostly above the blue ones. And we especially liked this plot because it also shows another bias, which is that the smokers who pay the highest charges are those with a large BMI. But for our study we’ll focus only on the fairness regarding the groups of smokers and non-smokers. Then, with this huge bias, we were curious to see how good the algorithm from the paper would be at making a predictor fair.</p> <h3 id="training-a-first-unfair-predictor">Training a first unfair predictor</h3> <p>What we needed first was a base predictor, and to get one we used simple Machine Learning models from <code class="language-plaintext highlighter-rouge">scikit-learn</code>.</p> <p>To compare the performances of these models, we used RMSE, which stands for Relative Mean Squared Error, and is defined as such : take the mean squared error of your predictor, and divide it by the variance of the groundtruth labels (which is in other words the mean squared error you would get with a predictor that always predicts the average of the groundtruth labels).</p> \[\text{RMSE}\:(g)=\frac{\displaystyle\sum_{(x,y)\in\mathcal{D}}(y-g(x))^2}{\displaystyle\sum_{(x,y)\in\mathcal{D}}(y-\overline{y})^2}\] <p>With this error function, the best model we found is a gradient boosting regressor, which gave us an RMSE of 0.34.</p> <h3 id="building-the-fair-predictor">Building the fair predictor</h3> <p>Now that we have this base model, we can move on to building the fair predictor using the post-processing algorithm from the paper <d-cite key="fairness"></d-cite>.</p> <p>Then, to evaluate fairness, we use the Demographic Parity distance defined earlier : the maximum difference between the CDFs on each sensitive group.</p> \[DP(g;s,s')=\sup_{t\in\mathbb{R}}\left|\mathbb{P}(g(X,S)\leq t|S=s)-\mathbb{P}(g(X,S)\leq t|S=s')\right|\] <p>So a fair predictor will have a DP distance close to zero, meaning that the CDFs will be the same for the different sensitive groups. And the link with CDFs gives us a good way to visualize fairness.</p> <h3 id="results">Results</h3> <p>So we plotted the empirical CDFs of the predictions on the two sensitive groups separately (smokers and non smokers).</p> <div class="row justify-content-center"> <div class="col-sm-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/insurance-fairness-480.webp 480w,/assets/img/fairness/insurance-fairness-800.webp 800w,/assets/img/fairness/insurance-fairness-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/fairness/insurance-fairness.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>In dark blue and dark red, we have the ECDFs for the base model predictions, respectively for non smokers and smokers : we can see that the DP distance is close to 1, meaning that this predictor is very unfair. This is normal since it was trained on highly biased data. Then, in light blue and orange, we have the ECDFs for the post-processed model predictions, respectively for non smokers and smokers : they are much closer to one another, the distance is around 0.2, so we did gain a lot of fairness. But we can see that the blue curve remains mostly above the orange one, meaning that the predictor is still biased.</p> <p>Now, seeing this plot, we wondered what would happen if we made an interpolation between those two states, how it would impact fairness.</p> <h3 id="interpolation">Interpolation</h3> <p>So that’s we did next, we made a simple linear interpolation between the model predictions and the post-processed predictions, with a parameter alpha representing the weight given to the fair predictions.</p> <figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="nx">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="nx">predictions</span> <span class="o">+</span> <span class="nx">alpha</span> <span class="o">*</span> <span class="nx">fair_predictions</span></code></pre></figure> <p>What we expected from the paper was to reach optimal fairness at alpha=1 (because that’s what theory says).</p> <p>But given the remaining bias we saw on the previous plot, we were also curious to see what would happen for alpha larger than 1, if we pushed the interpolation a bit further.</p> <div class="row justify-content-center"> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/interp1-480.webp 480w,/assets/img/fairness/interp1-800.webp 800w,/assets/img/fairness/interp1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/fairness/interp1.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/interp2-480.webp 480w,/assets/img/fairness/interp2-800.webp 800w,/assets/img/fairness/interp2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/fairness/interp2.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>that’s what we did, and we found out that optimal fairness was reached for alpha larger than one : indeed, we get a DP distance of 0.17 with alpha=1, and 0.11 with alpha=1.04, which is the optimal value.</p> <h3 id="fairness-vs-accuracy">Fairness vs. Accuracy</h3> <p>Finally, to better see what happens, we plotted the DP distance versus the RMSE, colored by the value of alpha :</p> <div class="row justify-content-center"> <div class="col-sm-8"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/fairness-accuracy-480.webp 480w,/assets/img/fairness/fairness-accuracy-800.webp 800w,/assets/img/fairness/fairness-accuracy-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/fairness/fairness-accuracy.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>first we see that when improving fairness (going down the y axis), we loose accuracy - which we expected.</p> <p>But also, you can clearly see here that alpha=1 is not optimal, and that the fairness distance keeps decreasing a bit, before going up again.</p> <p>This was not discussed in the paper at all, and we believe there could be further experiments to have a deeper understanding of what happens here.</p> <p>So now that we’ve seen our results on this first dataset with a very strong bias, I’ll let Martin take over for the results on our second dataset which is quite different.</p> <h2 id="income-dataset">Income dataset</h2> <h3 id="data-visualization-1">Data visualization</h3> <p>Goal : predict the salary Groups considered for fairness evaluation : male and female</p> <div class="row justify-content-center mt-3"> <div class="col-sm-8"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/income-dataset-480.webp 480w,/assets/img/fairness/income-dataset-800.webp 800w,/assets/img/fairness/income-dataset-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/fairness/income-dataset.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row justify-content-center mt-3"> <div class="col-sm-8"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/income-viz-480.webp 480w,/assets/img/fairness/income-viz-800.webp 800w,/assets/img/fairness/income-viz-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/fairness/income-viz.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The data is still biased, but less biased than in the insurance dataset</p> <p>Let’s see how it impacts fairness of the predictor</p> <div class="row justify-content-center"> <div class="col-sm-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/income-comparison-480.webp 480w,/assets/img/fairness/income-comparison-800.webp 800w,/assets/img/fairness/income-comparison-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/fairness/income-comparison.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Model predictions are fairer than on the insurance dataset, and the postprocessing seems to achieve better final fairness</p> <h2 id="conclusion">Conclusion</h2> <div class="row justify-content-center"> <div class="col-sm-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/income-insurance-comp-480.webp 480w,/assets/img/fairness/income-insurance-comp-800.webp 800w,/assets/img/fairness/income-insurance-comp-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/fairness/income-insurance-comp.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>trade-off accuracy vs. fairness</strong></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/fairness.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Marc Boëlle. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-projects",title:"projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"My GitHub profile and public repositories.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in '_pages/cv.md'. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-modeling-long-term-energy-storage-in-the-united-states",title:"Modeling long-term energy storage in the United States",description:"From my 4-months research internship at Lawrence Berkeley National Laboratory (LBNL), in the Grid Integration Group",section:"Posts",handler:()=>{window.location.href="/blog/2024/LDES/"}},{id:"post-statistical-studies-on-four-datasets-course-final-project",title:"statistical studies on four datasets - course final project",description:"GARCH and Hawkes processes, copulas, time series analysis. Project report from the course MAP565 at \xc9cole Polytechnique",section:"Posts",handler:()=>{window.location.href="/assets/pdf/stats-report.pdf"}},{id:"post-fairness-with-wasserstein-barycenters",title:"fairness with Wasserstein barycenters",description:"Fair Regression with Wasserstein Barycenters by Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, and Massimiliano Pontil",section:"Posts",handler:()=>{window.location.href="/blog/2023/fairness/"}},{id:"post-accelerated-gossip",title:"accelerated gossip",description:"Accelerated Gossip in Networks of Given Dimension using Jacobi Polynomial Iterations by Rapha\xebl Berthier, Francis Bach and Pierre Gaillard",section:"Posts",handler:()=>{window.location.href="/blog/2023/gossip/"}},{id:"post-weakly-supervised-image-classification-challenge",title:"weakly-supervised image classification challenge",description:"Kaggle challenge from the course INF473V (Deep Learning in Computer Vision) at \xc9cole Polytechnique",section:"Posts",handler:()=>{window.location.href="/assets/pdf/modal-report.pdf"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6D%61%72%63.%62%6F%65%6C%6C%65**%6E%6F-%73%70%61%6D**@%70%6F%6C%79%74%65%63%68%6E%69%71%75%65.%65%64%75","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/marcboelle","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/marcboelle","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>