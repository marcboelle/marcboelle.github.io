<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://marcboelle.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://marcboelle.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-09T13:45:51+00:00</updated><id>https://marcboelle.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Geometric Matrix Completion with Recurrent Multi-Graph Neural Network</title><link href="https://marcboelle.github.io/blog/2024/GDA/" rel="alternate" type="text/html" title="Geometric Matrix Completion with Recurrent Multi-Graph Neural Network"/><published>2024-12-05T00:00:00+00:00</published><updated>2024-12-05T00:00:00+00:00</updated><id>https://marcboelle.github.io/blog/2024/GDA</id><content type="html" xml:base="https://marcboelle.github.io/blog/2024/GDA/"><![CDATA[<p>Project report available <a href="/assets/pdf/GDA_report.pdf">here</a></p> <p></p>]]></content><author><name></name></author><category term="project"/><category term="graph"/><category term="convolution"/><category term="network,"/><category term="geometric"/><category term="data"/><summary type="html"><![CDATA[Project for Master MVA's Geometric Data Analysis course, in a group of 3 with Antonin Barbe and Zoé Herson.]]></summary></entry><entry><title type="html">Modeling long-term energy storage in the United States</title><link href="https://marcboelle.github.io/blog/2024/LDES/" rel="alternate" type="text/html" title="Modeling long-term energy storage in the United States"/><published>2024-07-30T00:00:00+00:00</published><updated>2024-07-30T00:00:00+00:00</updated><id>https://marcboelle.github.io/blog/2024/LDES</id><content type="html" xml:base="https://marcboelle.github.io/blog/2024/LDES/"><![CDATA[<p>internship report available <a href="/assets/pdf/LDES_report.pdf">here</a></p> <p></p>]]></content><author><name></name></author><category term="internship"/><category term="berkeley"/><category term="research"/><category term="modeling"/><category term="energy"/><category term="storage"/><category term="optimization"/><summary type="html"><![CDATA[From my 4-months research internship at Lawrence Berkeley National Laboratory (LBNL), in the Grid Integration Group]]></summary></entry><entry><title type="html">statistical studies on four datasets - course final project</title><link href="https://marcboelle.github.io/blog/2023/MAP565/" rel="alternate" type="text/html" title="statistical studies on four datasets - course final project"/><published>2023-12-08T00:00:00+00:00</published><updated>2023-12-08T00:00:00+00:00</updated><id>https://marcboelle.github.io/blog/2023/MAP565</id><content type="html" xml:base="https://marcboelle.github.io/blog/2023/MAP565/"><![CDATA[]]></content><author><name></name></author><category term="implementation"/><category term="pdf-report"/><category term="polytechnique"/><category term="math"/><category term="dataset"/><summary type="html"><![CDATA[GARCH and Hawkes processes, copulas, time series analysis. Project report from the course MAP565 at École Polytechnique]]></summary></entry><entry><title type="html">accelerated gossip</title><link href="https://marcboelle.github.io/blog/2023/gossip/" rel="alternate" type="text/html" title="accelerated gossip"/><published>2023-12-05T00:00:00+00:00</published><updated>2023-12-05T00:00:00+00:00</updated><id>https://marcboelle.github.io/blog/2023/gossip</id><content type="html" xml:base="https://marcboelle.github.io/blog/2023/gossip/"><![CDATA[<blockquote> <p>Final project for MAP578 course (Emerging Topics in Machine Learning: Collaborative Learning) at École Polytechnique. Paper study and implementation. Group project with Marc Boëlle and Antoine Millet.</p> </blockquote> <p><strong>Gossip problem framework</strong>: Given a network of agents represented as a graph, in which each agent holds a real value, we want to estimate the average of these values in a distributed manner. The agents (nodes) are related through links (edges), which they can use to communicate.</p> <div class="row justify-content-center"> <div class="col-sm-7"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gossip/graph-intro-480.webp 480w,/assets/img/gossip/graph-intro-800.webp 800w,/assets/img/gossip/graph-intro-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gossip/graph-intro.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Overview</strong> : The authors build a method for solving this problem that depends only on what they define as the <em>spectral dimension of the network</em>, which is intuitively the dimension of the space in which the agents live (for a grid in \(\mathbb{Z}^n\), the spectral dimension is \(n\)). Their method shows improvement over previous algorithms in the non-asymptotic regime, when the values are far from fully mixed.</p> <h2 id="the-gossip-problem-and-gossip-algorithms">The gossip problem and gossip algorithms</h2> <h3 id="the-gossip-problem">The gossip problem</h3> <p>The gossip problem, also known as the averaging problem, is fundamental in distributed networks, when information is shared among several machines without a central server. In this framework, we assume that each machine can communicate only with a few others, and these links are represented in a graph \(G=(V,E)\). Each machine hold an initial real value that the others do not know. The goal is then to find a way to get each machine to know the average of these initial values, through an iterative communication procedure and as quickly as possible.</p> <h4 id="problem-setting">Problem Setting</h4> <p>We are given an undirected finite graph \(G=(V,E)\), where \(V\) is the set of agents (nodes) in the network, and \(E\) the set of communication links (edges). Each node \(v\in V\) receives an initial real value \(\xi_v\) called an observation. We denote by \(\xi=(\xi_v)_{v\in V}\) the vector of observations, and the goal is then for each machine to know \(\bar\xi=\frac{1}{|V|}\sum_{v\in V}\xi_v\) through an iterative algorithm.</p> <p>\(x^t=(x^t_v)_{v\in V}\) denotes the values at time \(t\).</p> <p>NB : we are in in a synchronous framework, meaning that at every time step, all communciations are made at the same time (and not successively)</p> <h3 id="simple-gossip-algorithm">Simple gossip algorithm</h3> <p>Referred to as <em>simple gossip</em> in the paper, the landmark algorithm consists in the following intuitive scheme: at each iteration, each agent replaces his current value by an average of the values held by its neighbors. Formally, this means that we have a <strong>gossip matrix</strong> \(W\), which is stochastic, symmetric, nonnegative and supported by the graph : \(W_{u,v}&gt;0\implies \{u,v\}\in E\).</p> <details><summary>usual gossip matrix</summary> <p>If all vertices have degree smaller than \(d_{\max}\): \(W=I+\frac{1}{d_{\text{max}}}(A-D)\)</p> </details> <p>Then the simple gossip algorithm consists in the updates \(x^{t+1}=Wx^t\), leading to \(x^t=W^t\xi\).</p> <h3 id="shift-register-gossip-algorithm">Shift-register gossip algorithm</h3> <p>The idea of this second gossip algorithm is to take a linear combination of past iterates : \(x^{t+1}=\omega Wx^t+(1-\omega)x^{t-1}\) with \(x^0=\xi\) and \(x^1=W\xi\) Without going into the details of this algorithm, it is clear that the parameter \(\omega\) plays an important role and must be finetuned.</p> <p>It was shown in <d-cite key="omega"></d-cite> that \(\omega=2\frac{1-\sqrt{\gamma(1-\gamma/4)}}{(1-\gamma/2)^2}\), which depends on the spectral gap \(\gamma\), works well, with an acceleration of convergence for small gammas (compared to simple gossip).</p> <p>This is a nice improvement since \(\gamma\) is indeed very small in large graphs. And in practicle cases, graph networks are quite big, so it is a worthy improvement.</p> <p>Yet one important point here is that <strong>we assume that all agents know the spectral gap</strong> \(\gamma\). But in the general case, there is no reason why they should. Thus, the point of this paper is to find a way to avoid this difficulty by building a method based not on \(\gamma\) but on the spectral dimension of the graph.</p> <h2 id="polynomial-gossip">Polynomial gossip</h2> <p>Intuitively, polynomial gossip consists in replacing \(x^t=W^t\xi\) by \(x^t=P_t(W)\xi\). In other words, at every time step, we take linear combinations of all past iterates of the simple gossip method.</p> <p>The polynomials consider must verify \(\text{deg}P_t\leq t\) (to ensure that the \(x^t\) can be computed using at most \(t\) communication steps) and \(P_t(1)=1\) (to ensure that if all initial observations are equal, then \(x^t=\xi\) for any \(t\)).</p> <p>Diagonalizing \(W\), with \(1=\lambda_1&gt;\lambda_2&gt;\lambda_n&gt;-1\), the simple gossip formula can be rewritten as :</p> \[x^t = U \begin{pmatrix} 1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda_2^t &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \lambda_n^t \end{pmatrix} U^T \xi\] <p>And same for the polynomial gossip formula:</p> \[x^t = U \begin{pmatrix} P_t(1) &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; P_t(\lambda_2) &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; P_t(\lambda_n) \end{pmatrix} U^T \xi\] <h3 id="finding-the-optimal-polynomials">Finding the optimal polynomials</h3> <h4 id="error-to-measure-performance">Error to measure performance</h4> \[\mathcal{E}(P_t)=\|x^t-\bar\xi 1\|^2=\|P_t(W)\xi-\bar\xi 1\|^2\] \[\mathcal{E}(P_t)=\int_{-1}^1P_t(\lambda)^2d\sigma (\lambda)\] <p>with \(d\sigma (\lambda)=\sum_{i=2}^n\langle\xi,u^i\rangle^2\delta_{\lambda_i}\) the spectral measure</p> <h4 id="optimization">Optimization</h4> <p>We want to find \(\pi_t\) such that:</p> \[\pi_t\in\underset{P_t(1)=1,\deg P_t\leqslant t}{\text{argmin}}\mathcal{E}(P_t)\] <p>So the optimal polynomial \(\pi_t\) verifies:</p> \[\pi_t\in\underset{P_t(1)=1,\deg P_t\leqslant t}{\text{argmin}}\int_{-1}^1P_t(\lambda)^2d\sigma (\lambda)\] <p>Final goal : minimize this error, under the conditions on degree and value in 1.</p> <h3 id="some-results-on-orthogonal-polynomials">Some results on orthogonal polynomials</h3> <details><summary>theorem: orthogonal polynomials</summary> <p>\(\pi_t\) is unique and \(\pi_0,\pi_1,\dots\) is the unique sequence of orthogonal polynomials w.r.t. \(\tau\) defined as follows, and normalized such that \(\pi_t(1)=1\) :</p> \[d\tau(\lambda)=(1-\lambda)d\sigma(\lambda)\] </details> <details><summary>theorem: second order recursion</summary> <p>Let \(\pi_0,\pi_1,\dots\) be a sequence of orthogonal polynomials w.r.t. some measure \(\tau\). There exist three sequences of coefficients \((a_t)\), \((b_t)\) and \((c_t)\) such that :</p> \[\pi_{t+1}(\lambda)=(a_t\lambda+b_t)\pi_t(\lambda)-c_t\pi_{t-1}(\lambda)\] </details> <p>We know that our optimal polynomial \(\pi_t\) minimizes this integral w.r.t. the spectral measure sigma</p> <p>Now we know from the theorem above that \(\pi_t\) defined as such is unique, and that the sequence \(\pi_0, \pi_1\cdots\) is the unique sequence of orthogonal polynomials wrt the measure \(\tau\) defined here, and normalized to satisfy our condition of value in 1</p> <p>We won’t need to know much about orthogonal polynomials, except that they always follow a second order recursion (see theorem above) with coefficients \(a_t,b_t,c_t\).</p> <h3 id="back-to-gossip">Back to gossip</h3> <p>Thus we get the following formula :</p> \[x^t=\pi_t(W)\xi\] <p>And from the second order theorem :</p> \[\begin{align*} &amp;x^{t+1} = a_t W x^t + b_t x^t - c_t x^{t-1} \\ &amp;x^1 = a_0 W \xi + b_0 \xi \\ &amp;x^0 = \xi \end{align*}\] <p><strong>Conclusion</strong>: the best polynomial gossip algorithm is a <strong>second order method</strong>. This is good in practice, because it implies a limited memory cost to store past values.</p> <p><strong>Problem</strong> : the polynomials are hard to compute because the spectral measure \(\sigma\) is unknown in pratice. So the idea is to approximate \(\sigma\) with a simpler measure \(\tilde\sigma\). =&gt; How do we choose \(\tilde\sigma\) ?</p> <h3 id="approximating-the-spectral-measure">Approximating the spectral measure</h3> <p>Based on the notion of spectral dimension introduced in the paper <d-cite key="gossip"></d-cite>, the authors decide to take the following approximating measure: \(d\tilde{\sigma}(\lambda) = (1 - \lambda)^{d/2 - 1} \mathbb{1}_{\{\lambda \in (-1, 1)\}} \, d\lambda\)</p> <p>With the \(\tau\) notation previously introduced for orthogonal polynomials, we end up having to determine the sequence of orthogonal polynomials associated to the following measure: \(d\tilde{\tau}(\lambda) = (1 - \lambda)^{d/2} \mathbb{1}_{\{\lambda \in (-1,1)\}} \, d\lambda\)</p> <p>And this sequence turns out to be the <strong>Jacobi polynomials</strong>!</p> <div class="row justify-content-center mt-3"> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gossip/jacobi-comp-480.webp 480w,/assets/img/gossip/jacobi-comp-800.webp 800w,/assets/img/gossip/jacobi-comp-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gossip/jacobi-comp.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <details><summary>formulas for the coefficients of the Jacobi polynomials</summary> \[\begin{align*} a_0^{(d)} &amp;= \frac{d + 4}{2(2 + d)}, \\ a_t^{(d)} &amp;= \frac{(2t + d/2 + 1)(2t + d/2 + 2)}{2(t + 1 + d/2)^2}, \\ c_t^{(d)} &amp;= \frac{t^2(2t + d/2 + 2)}{(t + 1 + d/2)^2(2t + d/2)}, \\ b_0^{(d)} &amp;= \frac{d}{2(2 + d)}, \\ b_t^{(d)} &amp;= \frac{d^2(2t + d/2 + 1)}{8(t + 1 + d/2)^2(2t + d/2)}. \end{align*}\] </details> <h2 id="simulations">Simulations</h2> <p>Moving on to simulations, we systematically compared the performances of the different methods we’ve presented : <strong>simple gossip</strong>, <strong>shift-register</strong>, and <strong>Jacobi iterations</strong>. We decided to focus on 2-dimensional grids, and to visualize them as images in which each pixel represents a node and its color represents the value it holds.</p> <h3 id="normal-distribution">Normal distribution</h3> <p>For the initial distribution, the authors decided to use only \(\xi\sim\mathcal{N}(0,1)\). So we decided to first see how the algorithms performed on this distribution, and then to try them on a completely different one.</p> <p>First with the normal distribution, we can see indeed that the Jacobi method converges faster than the other two.</p> <p>To evaluate performance more precisely, we plotted the error as a function of time, both in linear and log scale :</p> <div class="row justify-content-center"> <div class="col-sm-11"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gossip/error-normal-480.webp 480w,/assets/img/gossip/error-normal-800.webp 800w,/assets/img/gossip/error-normal-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gossip/error-normal.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The log scale provides a more precise understanding of the error : Jacobi is best at first, and then shift-register achieves better results, but this is when the error is already very small (\(10^{-3}\)) after many iterations. In practical cases we won’t necessarily need this extra precision, so fast convergence in short term will often be good enough.</p> <h3 id="step-distribution">Step distribution</h3> <div class="row justify-content-center"> <div class="col-sm-11"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gossip/error-step-480.webp 480w,/assets/img/gossip/error-step-800.webp 800w,/assets/img/gossip/error-step-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gossip/error-step.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="paper-study"/><category term="gossip"/><category term="math"/><category term="polytechnique"/><summary type="html"><![CDATA[Accelerated Gossip in Networks of Given Dimension using Jacobi Polynomial Iterations by Raphaël Berthier, Francis Bach and Pierre Gaillard]]></summary></entry><entry><title type="html">fairness with Wasserstein barycenters</title><link href="https://marcboelle.github.io/blog/2023/fairness/" rel="alternate" type="text/html" title="fairness with Wasserstein barycenters"/><published>2023-12-05T00:00:00+00:00</published><updated>2023-12-05T00:00:00+00:00</updated><id>https://marcboelle.github.io/blog/2023/fairness</id><content type="html" xml:base="https://marcboelle.github.io/blog/2023/fairness/"><![CDATA[<blockquote> <p>Final project for MAP588 course (Emerging Topics in Machine Learning: Optimal Transport) at École Polytechnique. Paper study and implementation. Group project with Marc Boëlle and Martin Morange.</p> </blockquote> <h2 id="results-of-the-paper">Results of the paper</h2> <h3 id="problem-setting">Problem setting</h3> <p>The authors focus on regression problems \(Y=f^*(X,S)+\xi\), where \(f^*\) is the regression function minimizing the squared risk, \(S\) a sensitive attribute and \(\xi\) a (centered) noise.</p> <h3 id="demographic-parity-dp">Demographic Parity (DP)</h3> <p>To evaluate the fairness of some predictors, there are different metrics available. Demographic Parity is one of them and extensively used in the literature, in particular in this paper. Intuitively, it is satisfied if the distribution of the prediction is independent of the sensitive attribute.</p> <p>Formally, with \(S\) the attribute representing the sensitive groups in the population, a predictor \(g\) is fair (according to the DP criterion) if</p> \[\forall (s,s')\in S^2,\:\:\sup_{t\in\mathbb{R}}\left|\mathbb{P}(g(X,S)\leq t|S=s)-\mathbb{P}(g(X,S)\leq t|S=s')\right|=0\] <p>In other words, the Kolmogorov-Smirnov distance between the distributions on all sensitive groups must be \(0\).</p> <h3 id="strategy-of-the-authors">Strategy of the authors</h3> <p>From the optimal predictor \(f^*\), the authors suggests that we build a fair predictor \(g^*\) (w.r.t. DP) which remains “close” to \(f^*\) - to keep the accuracy as high as possible. We end up with this new problem :</p> \[\min_{g\text{ is fair}}\mathbb{E}\left(f^*(X,S)-g(X,S)\right)^2\] <p>Using Optimal Transport theory, the authors provide a closed form expression of \(g^*\) using Wasserstein barycenters :</p> <p>Theorem:</p> \[\min_{g \text{ is fair}} \mathbb{E}(f^*(X,S) - g(X,S))^2 = \min_{\nu} \sum_{s \in S} p_s \mathcal{W}_2^2(\nu_{f^*|s}, \nu)\] \[g^*(x,s) = \left( \sum_{s' \in S} p_{s'} Q_{f^*|s'} \right) \circ F_{f^*|s}(f^*(x,s))\] <p>with</p> \[\mathcal{W}_2^2(\mu, \nu) = \inf_{\gamma \in \Gamma_{\mu, \nu}} \int_{\gamma \in \Gamma_{\mu, \nu}} |x - y|^2 d\gamma(x,y)\] <table> <tbody> <tr> <td>with $$F_{f</td> <td>s}\(the CDF of\)f\(,\)Q_{f</td> <td>s}\(its quantile function and\)p_s = \mathbb{P}(S=s)$$.</td> </tr> </tbody> </table> <h2 id="insurance-dataset">Insurance dataset</h2> <h3 id="data-visualization">Data visualization</h3> <p>The first dataset we worked on is the insurance dataset. Each row corresponds to an individual, and our goal is to predict the amount of charges people have to pay for medical insurance. The sensitive groups we chose to consider for fairness evaluation are smokers and non-smokers. You may guess that smokers will have to pay higher charges for medical insurance than non-smokers. And indeed they do:</p> <div class="row justify-content-center"> <div class="col-sm-8"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/insurance-dataset-480.webp 480w,/assets/img/fairness/insurance-dataset-800.webp 800w,/assets/img/fairness/insurance-dataset-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/fairness/insurance-dataset.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Of all the plots we made to visualize the data, we liked the following one :</p> <div class="row justify-content-center"> <div class="col-sm-8"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/insurance-viz-480.webp 480w,/assets/img/fairness/insurance-viz-800.webp 800w,/assets/img/fairness/insurance-viz-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/fairness/insurance-viz.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It’s a scatter plot for charges versus BMI (Body Mass Index), and we’ve colored the points according to the sensitive attributes – smokers are in red, and non-smokers are in blue. You can clearly see the bias in the data, the red points being mostly above the blue ones. And we especially liked this plot because it also shows another bias, which is that the smokers who pay the highest charges are those with a large BMI. But for our study we’ll focus only on the fairness regarding the groups of smokers and non-smokers. Then, with this huge bias, we were curious to see how good the algorithm from the paper would be at making a predictor fair.</p> <h3 id="training-a-first-unfair-predictor">Training a first unfair predictor</h3> <p>What we needed first was a base predictor, and to get one we used simple Machine Learning models from <code class="language-plaintext highlighter-rouge">scikit-learn</code>.</p> <p>To compare the performances of these models, we used RMSE, which stands for Relative Mean Squared Error, and is defined as such : take the mean squared error of your predictor, and divide it by the variance of the groundtruth labels (which is in other words the mean squared error you would get with a predictor that always predicts the average of the groundtruth labels).</p> \[\text{RMSE}\:(g)=\frac{\displaystyle\sum_{(x,y)\in\mathcal{D}}(y-g(x))^2}{\displaystyle\sum_{(x,y)\in\mathcal{D}}(y-\overline{y})^2}\] <p>With this error function, the best model we found is a gradient boosting regressor, which gave us an RMSE of 0.34.</p> <h3 id="building-the-fair-predictor">Building the fair predictor</h3> <p>Now that we have this base model, we can move on to building the fair predictor using the post-processing algorithm from the paper <d-cite key="fairness"></d-cite>.</p> <p>Then, to evaluate fairness, we use the Demographic Parity distance defined earlier : the maximum difference between the CDFs on each sensitive group.</p> \[DP(g;s,s')=\sup_{t\in\mathbb{R}}\left|\mathbb{P}(g(X,S)\leq t|S=s)-\mathbb{P}(g(X,S)\leq t|S=s')\right|\] <p>So a fair predictor will have a DP distance close to zero, meaning that the CDFs will be the same for the different sensitive groups. And the link with CDFs gives us a good way to visualize fairness.</p> <h3 id="results">Results</h3> <p>So we plotted the empirical CDFs of the predictions on the two sensitive groups separately (smokers and non smokers).</p> <div class="row justify-content-center"> <div class="col-sm-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/insurance-fairness-480.webp 480w,/assets/img/fairness/insurance-fairness-800.webp 800w,/assets/img/fairness/insurance-fairness-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/fairness/insurance-fairness.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In dark blue and dark red, we have the ECDFs for the base model predictions, respectively for non smokers and smokers : we can see that the DP distance is close to 1, meaning that this predictor is very unfair. This is normal since it was trained on highly biased data. Then, in light blue and orange, we have the ECDFs for the post-processed model predictions, respectively for non smokers and smokers : they are much closer to one another, the distance is around 0.2, so we did gain a lot of fairness. But we can see that the blue curve remains mostly above the orange one, meaning that the predictor is still biased.</p> <p>Now, seeing this plot, we wondered what would happen if we made an interpolation between those two states, how it would impact fairness.</p> <h3 id="interpolation">Interpolation</h3> <p>So that’s we did next, we made a simple linear interpolation between the model predictions and the post-processed predictions, with a parameter alpha representing the weight given to the fair predictions.</p> <figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="nx">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="nx">predictions</span> <span class="o">+</span> <span class="nx">alpha</span> <span class="o">*</span> <span class="nx">fair_predictions</span></code></pre></figure> <p>What we expected from the paper was to reach optimal fairness at alpha=1 (because that’s what theory says).</p> <p>But given the remaining bias we saw on the previous plot, we were also curious to see what would happen for alpha larger than 1, if we pushed the interpolation a bit further.</p> <div class="row justify-content-center"> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/interp1-480.webp 480w,/assets/img/fairness/interp1-800.webp 800w,/assets/img/fairness/interp1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/fairness/interp1.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/interp2-480.webp 480w,/assets/img/fairness/interp2-800.webp 800w,/assets/img/fairness/interp2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/fairness/interp2.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>that’s what we did, and we found out that optimal fairness was reached for alpha larger than one : indeed, we get a DP distance of 0.17 with alpha=1, and 0.11 with alpha=1.04, which is the optimal value.</p> <h3 id="fairness-vs-accuracy">Fairness vs. Accuracy</h3> <p>Finally, to better see what happens, we plotted the DP distance versus the RMSE, colored by the value of alpha :</p> <div class="row justify-content-center"> <div class="col-sm-8"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/fairness-accuracy-480.webp 480w,/assets/img/fairness/fairness-accuracy-800.webp 800w,/assets/img/fairness/fairness-accuracy-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/fairness/fairness-accuracy.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>first we see that when improving fairness (going down the y axis), we loose accuracy - which we expected.</p> <p>But also, you can clearly see here that alpha=1 is not optimal, and that the fairness distance keeps decreasing a bit, before going up again.</p> <p>This was not discussed in the paper at all, and we believe there could be further experiments to have a deeper understanding of what happens here.</p> <p>So now that we’ve seen our results on this first dataset with a very strong bias, I’ll let Martin take over for the results on our second dataset which is quite different.</p> <h2 id="income-dataset">Income dataset</h2> <h3 id="data-visualization-1">Data visualization</h3> <p>Goal : predict the salary Groups considered for fairness evaluation : male and female</p> <div class="row justify-content-center mt-3"> <div class="col-sm-8"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/income-dataset-480.webp 480w,/assets/img/fairness/income-dataset-800.webp 800w,/assets/img/fairness/income-dataset-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/fairness/income-dataset.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row justify-content-center mt-3"> <div class="col-sm-8"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/income-viz-480.webp 480w,/assets/img/fairness/income-viz-800.webp 800w,/assets/img/fairness/income-viz-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/fairness/income-viz.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The data is still biased, but less biased than in the insurance dataset</p> <p>Let’s see how it impacts fairness of the predictor</p> <div class="row justify-content-center"> <div class="col-sm-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/income-comparison-480.webp 480w,/assets/img/fairness/income-comparison-800.webp 800w,/assets/img/fairness/income-comparison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/fairness/income-comparison.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Model predictions are fairer than on the insurance dataset, and the postprocessing seems to achieve better final fairness</p> <h2 id="conclusion">Conclusion</h2> <div class="row justify-content-center"> <div class="col-sm-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fairness/income-insurance-comp-480.webp 480w,/assets/img/fairness/income-insurance-comp-800.webp 800w,/assets/img/fairness/income-insurance-comp-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/fairness/income-insurance-comp.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>trade-off accuracy vs. fairness</strong></p>]]></content><author><name></name></author><category term="paper-study"/><category term="fairness"/><category term="math"/><category term="wasserstein"/><category term="polytechnique"/><summary type="html"><![CDATA[Fair Regression with Wasserstein Barycenters by Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, and Massimiliano Pontil]]></summary></entry><entry><title type="html">weakly-supervised image classification challenge</title><link href="https://marcboelle.github.io/blog/2023/modal/" rel="alternate" type="text/html" title="weakly-supervised image classification challenge"/><published>2023-06-20T00:00:00+00:00</published><updated>2023-06-20T00:00:00+00:00</updated><id>https://marcboelle.github.io/blog/2023/modal</id><content type="html" xml:base="https://marcboelle.github.io/blog/2023/modal/"><![CDATA[]]></content><author><name></name></author><category term="challenge"/><category term="pdf-report"/><category term="deep-learning"/><category term="computer-vision"/><category term="polytechnique"/><summary type="html"><![CDATA[Kaggle challenge from the course INF473V (Deep Learning in Computer Vision) at École Polytechnique]]></summary></entry></feed>